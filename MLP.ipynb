{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'relu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-972478dbf0c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfuncoes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigmoid_derivative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelu_derivative\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'relu'"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "from funcoes import sigmoid, sigmoid_derivative, relu, relu_derivative\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CONFIGURAÇÕES: \n",
    "\n",
    "quantidade de camadas,\n",
    "quantidade de neurônios em cada camada,\n",
    "função de ativação\n",
    "\"\"\"\n",
    "\n",
    "dimensoes_camadas = [2,2,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dados = [\n",
    "    [10,10],\n",
    "    [10,13],\n",
    "    [13,10],\n",
    "    \n",
    "    [1,1],\n",
    "    [-2,1],\n",
    "    [1,-2],\n",
    "    \n",
    "#     [1,10],\n",
    "#     [1,13],\n",
    "#     [-2,10]\n",
    "]\n",
    "\n",
    "labels = [\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    \n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    \n",
    "#     2,\n",
    "#     2,\n",
    "#     2\n",
    "]\n",
    "\n",
    "\n",
    "x = [d[0] for d in dados]\n",
    "y = [d[1] for d in dados]\n",
    "plt.scatter(x,y, c=labels)\n",
    "plt.title(\"Dados de brincadeira\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y, tam=2):\n",
    "    \"\"\"\n",
    "    Serve para converter uma clase em um vetor. Nesse vetor, uma das posições\n",
    "    é o inteiro 1, e o resto é 0.\n",
    "    \n",
    "    Exemplos:\n",
    "    a classe 0 vira [1, 0, 0]\n",
    "    a classe 1 vira [0, 1, 0]\n",
    "    a classe 2 vira [0, 0, 1]\n",
    "    \n",
    "    se eu jogar um input no MLP, e eu sei que pra esse input a resposta certa\n",
    "    é da classe 2, então eu vou dizer que na saída eu espero que tenha [0, 0, 1]\n",
    "    \"\"\"\n",
    "    \n",
    "    resultado = [0] * tam\n",
    "    \n",
    "    resultado[y] = 1\n",
    "    \n",
    "    return resultado\n",
    "\n",
    "def erro_total(a: np.array, b: np.array):\n",
    "    \"\"\"\n",
    "    calcula o erro total entre a e b. Um deles deve ser uma saída esperada,\n",
    "    e o outro deve ser a saída obtida.\n",
    "    \n",
    "    O erro total é a somatória do quadrado das diferenças, algo como:\n",
    "    (a1 - b1)^2 + (a2 - b2)^2 + ...\n",
    "    \"\"\"\n",
    "    \n",
    "    return (a-b)**2\n",
    "    \n",
    "\n",
    "def gerar_um_peso():\n",
    "    \"\"\"Sorteia um número entre -1 e 1\"\"\"\n",
    "    sinal, numero = random(), random()\n",
    "    \n",
    "    if sinal >= 0.5:\n",
    "        return numero\n",
    "    else:\n",
    "        return -numero\n",
    "    \n",
    "def gerar_N_pesos(n: int):\n",
    "    \"\"\"Gera um array de números sorteados\"\"\"\n",
    "    return np.array([gerar_um_peso() for i in range(n)])\n",
    "\n",
    "def gerar_rede(dimensoes: list, biases: list = None):\n",
    "    \"\"\"\n",
    "    Recebe a quantidade de neurônios em cada camada. Recebe uma lista de booleanos\n",
    "    indicando se deve ter um bias em cada camada de pesos. Se nada for passado,\n",
    "    assumimos que é pra ter bias em todas camadas.\n",
    "    Gera pesos aleatoriamente.\n",
    "    \n",
    "    Observação: camadas de neurônios é uma coisa, camada de pesos é outra\n",
    "    se eu tiver duas camadas de neurônios, eu tenho uma camada de pesos, que \n",
    "    liga essas duas camadas de neurônios\n",
    "    \n",
    "    O retorno é no formato\n",
    "    \n",
    "    [ <-- no primeiro nível, temos uma lista de camadas de pesos\n",
    "    \n",
    "        [ <-- para cada camada, temos uma lista de neurônios\n",
    "        \n",
    "            [ <-- para cada neurônio, temos uma lista de pesos\n",
    "            \n",
    "                0.2, 0.1, ...\n",
    "            ]\n",
    "        ]\n",
    "    ]\n",
    "    \n",
    "    exemplo: gerar_rede([1,1]) gera pesos para uma rede com duas camadas de neurônios,\n",
    "    com um neurônio em cada camada. Se temos duas camadas de neurônios, vamos ter uma camada\n",
    "    de pesos.\n",
    "    \n",
    "    Resultado:\n",
    "    rede = gerar_rede([1,1])  # [[[0.5]]], que é uma lista de camadas\n",
    "    print(rede[0])            # [[0.5]]  , que é a lista de neurônios para a primeira camada\n",
    "    print(rede[0][0])         # [0.5]    , que é a lista de pesos para o primeiro neurônio na primeira camada\n",
    "    print(rede[0][0][0])      # 0.5      , que é o primeiro peso do primeiro neurônio da primeira camada\n",
    "    \"\"\"\n",
    "    rede = []\n",
    "    \n",
    "    if not biases:\n",
    "        biases = [True] * (len(dimensoes) - 1)\n",
    "    \n",
    "    for i in range(1, len(dimensoes)):\n",
    "        if biases[i-1]:\n",
    "            # se é pra ter bias nessa camada, eu gero um peso a mais\n",
    "            pesos = [ gerar_N_pesos(dimensoes[i-1] + 1) for _ in range(dimensoes[i]) ]\n",
    "        else:\n",
    "            # não é pra ter bias: eu não gero um peso a mais\n",
    "            pesos = [ gerar_N_pesos(dimensoes[i-1]) for _ in range(dimensoes[i]) ]\n",
    "            \n",
    "        rede.append( np.array(pesos) )\n",
    "        \n",
    "    return rede\n",
    "\n",
    "def printar_rede(rede: list):\n",
    "    \"\"\"Printa as camadas de pesos da rede\"\"\"\n",
    "    \n",
    "    for i,camada in enumerate(rede):\n",
    "        print(\"Camada %d:\" % i)\n",
    "        for j,neuronio in enumerate(camada):\n",
    "            print(\"\\tNeurônio %d:\" % j)\n",
    "            for w, peso in enumerate(neuronio):\n",
    "                print(\"\\t\\tPeso %d:  %.6f\" % (w, peso))\n",
    "                \n",
    "def forward(rede: list, entrada: np.array, biases: list = None):\n",
    "    \"\"\"\n",
    "    Pega a entrada, e faz todos os cálculos pra obter a saída. Recebe uma lista de booleanos\n",
    "    indicando se deve ter um bias em cada camada de pesos. Se nada for passado,\n",
    "    assumimos que é pra ter bias em todas camadas.\n",
    "    Assim, faz as multiplicações pelos pesos, joga nas funções\n",
    "    de ativação, e passa em todas camadas, pra retornar o que sobra\n",
    "    na camada de saída.\n",
    "    \n",
    "    O retorno é uma tupla.\n",
    "    \n",
    "    O primeiro elemento são os resultados das multiplicações\n",
    "    das entradas pelos pesos (para cada camada), antes de aplicar a função de ativação.\n",
    "    \n",
    "    O segundo elemento é a mesma coisa que o primeiro, só que após a aplicação na função\n",
    "    de ativação.\n",
    "    \"\"\"\n",
    "    \n",
    "    dados = entrada\n",
    "    \n",
    "    # Vai guardar os resultados das multiplicações das entradas pelos pesos\n",
    "    # (antes de jogar nas funções de ativação)\n",
    "    somas_ponderadas = []\n",
    "    # Vai guardar a mesma coisa que somas_ponderadas, só que depois de passar\n",
    "    # nas funções de ativação\n",
    "    ativacoes = []\n",
    "    \n",
    "    if not biases:\n",
    "        biases = [True] * (len(rede))\n",
    "    \n",
    "    for i,camada in enumerate(rede):\n",
    "        # Decide se inclui o bias\n",
    "        if biases[i]:\n",
    "            # insere o inteiro '1' na posição 0\n",
    "            dados = np.insert(dados, 0, 1)\n",
    "        \n",
    "        # Aplica a soma ponderada pelos pesos\n",
    "        dados = np.sum(dados * camada, axis=1)\n",
    "        somas_ponderadas.append(dados)\n",
    "        \n",
    "        # Aplica a função de ativação\n",
    "        dados = sigmoid(dados)\n",
    "        ativacoes.append(dados)\n",
    "    \n",
    "    return somas_ponderadas, ativacoes\n",
    "\n",
    "\n",
    "def backward(rede: list, somas_ponderadas: np.array, ativacoes: np.array, desejados: np.array):\n",
    "    \"\"\"\n",
    "    Realiza o backpropagation.\n",
    "    \n",
    "    Precisa informar os resultados desejados, para poder calcular o erro.\n",
    "    \n",
    "    ativacoes e somas_ponderadas são necessárias pra poder calcular o ajuste em cada camada\n",
    "    \n",
    "    rede é o que contém os pesos que deverão ser ajustados\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pegue as ativações da última camada, que é a de saída\n",
    "    saida = ativacoes[-1]\n",
    "    erro = erro_total(saida, desejados)\n",
    "    \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "rede = gerar_rede([2,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antes do backprop\n",
      "Camada 0:\n",
      "\tNeurônio 0:\n",
      "\t\tPeso 0:  0.031989\n",
      "\t\tPeso 1:  -0.808963\n",
      "\t\tPeso 2:  0.301146\n",
      "Camada 1:\n",
      "\tNeurônio 0:\n",
      "\t\tPeso 0:  -0.985362\n",
      "\t\tPeso 1:  -0.759468\n",
      "\tNeurônio 1:\n",
      "\t\tPeso 0:  0.903645\n",
      "\t\tPeso 1:  0.160493\n"
     ]
    }
   ],
   "source": [
    "print(\"antes do backprop\")\n",
    "printar_rede(rede)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depois do backprop\n",
      "Camada 0:\n",
      "\tNeurônio 0:\n",
      "\t\tPeso 0:  0.031989\n",
      "\t\tPeso 1:  -0.808963\n",
      "\t\tPeso 2:  0.301146\n",
      "Camada 1:\n",
      "\tNeurônio 0:\n",
      "\t\tPeso 0:  -0.841360\n",
      "\t\tPeso 1:  -0.758548\n",
      "\tNeurônio 1:\n",
      "\t\tPeso 0:  0.757636\n",
      "\t\tPeso 1:  0.159560\n"
     ]
    }
   ],
   "source": [
    "print(\"depois do backprop\")\n",
    "printar_rede(rede)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depois do backprop total\n",
      "Camada 0:\n",
      "\tNeurônio 0:\n",
      "\t\tPeso 0:  0.031147\n",
      "\t\tPeso 1:  -0.809191\n",
      "\t\tPeso 2:  0.300546\n",
      "Camada 1:\n",
      "\tNeurônio 0:\n",
      "\t\tPeso 0:  -0.841360\n",
      "\t\tPeso 1:  -0.758548\n",
      "\tNeurônio 1:\n",
      "\t\tPeso 0:  0.757636\n",
      "\t\tPeso 1:  0.159560\n"
     ]
    }
   ],
   "source": [
    "print(\"depois do backprop total\")\n",
    "printar_rede(rede)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.27086914 0.71190848]\n",
      "[1, 0]\n"
     ]
    }
   ],
   "source": [
    "indice_entrada = 0\n",
    "\n",
    "# I é a soma ponderada das entradas de uma camada\n",
    "# Y é a mesma coisa que o I, só que depois de passar\n",
    "# pelas funções de ativação\n",
    "I, Y = forward(rede, dados[indice_entrada])\n",
    "d = one_hot(labels[indice_entrada])\n",
    "print(Y[-1])\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_di = sigmoid_derivative # g_di é a derivada da função de ativação em relação a I\n",
    "nabla = 1 # taxa de aprendizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.30022874 0.68106196]\n",
      "[1, 0]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ajuste da última camada\n",
    "\"\"\"\n",
    "\n",
    "# vamos guardar todos os gradientes locais aqui. o primeiro item\n",
    "# são os gradientes da última camada, o segundo item da penúltima camada,\n",
    "# o terceiro da antepenúltima, etc...\n",
    "# isso é necessário porque, no backpropagation, o gradiente da última\n",
    "# camada é usado pra calcular o gradiente da penúltima, et cetera\n",
    "gradientes_locais = []\n",
    "\n",
    "# guardar só os gradientes da última camada\n",
    "ultimos_gradientes = []\n",
    "\n",
    "c = len(rede) - 1 # pegar a ultima camada\n",
    "\n",
    "for j in range(len(rede[c])):\n",
    "\n",
    "    gradiente_local_neuronio = (d[j] - Y[c][j]) * g_di(I[c][j])\n",
    "    \n",
    "    ultimos_gradientes.append(gradiente_local_neuronio)\n",
    "\n",
    "    for i in range(len(rede[c][j])):\n",
    "        if i == 0:\n",
    "            # quando i == 0, estamos ajustando o peso do bias para o neurônio j.\n",
    "            # nesse caso, fazemos esse * 1 no final\n",
    "            rede[c][j][i] = rede[c][j][i] + nabla * gradiente_local_neuronio * 1\n",
    "        else:\n",
    "            # esse é o ajuste nos demais pesos, sem ser do bias.\n",
    "            rede[c][j][i] = rede[c][j][i] + nabla * gradiente_local_neuronio * Y[c-1][i - 1]\n",
    "            \n",
    "gradientes_locais.append(ultimos_gradientes)\n",
    "                \n",
    "I2, Y2 = forward(rede, dados[indice_entrada])\n",
    "\n",
    "print(Y2[-1])\n",
    "print(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.30031698 0.68104277]\n",
      "[1, 0]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ajuste nas camadas escondidas\n",
    "\"\"\"\n",
    "\n",
    "# for _ in range(1):\n",
    "\n",
    "c = len(rede) - 2 # pegar a penultima camada\n",
    "\n",
    "# guardar os gradientes que calcularmos para a camada atual\n",
    "gradientes_escondidos = []\n",
    "\n",
    "for j in range(len(rede[c])):\n",
    "\n",
    "    # o gradiente desse neurônio é calculado com base: \n",
    "    # nos gradientes dos neurônios da camada da frente (gradientes_locais[c])\n",
    "    # nos pesos da camada da frente que se ligam com esse neurônio (rede[i+1][k][j+1])\n",
    "    somatorio = [gradientes_locais[c][k] * rede[c+1][k][j+1] for k in range(len(rede[c+1]))]\n",
    "    gradiente_local_neuronio = sum(somatorio) * g_di(I[c][j])\n",
    "    \n",
    "    # g_di(I[i][j]) é a derivada da função de ativação aplicada \n",
    "    # na soma ponderada da camada atual\n",
    "    gradientes_escondidos.append(gradiente_local_neuronio)\n",
    "\n",
    "    for i in range(len(rede[c][j])):\n",
    "        if i == 0:\n",
    "            # quando i == 0, estamos ajustando o peso do bias para o neurônio j.\n",
    "            # nesse caso, fazemos esse * 1 no final\n",
    "            rede[c][j][i] = rede[c][j][i] + nabla * gradiente_local_neuronio * 1\n",
    "        else:\n",
    "            # esse é o ajuste nos demais pesos, sem ser do bias.\n",
    "            rede[c][j][i] = rede[c][j][i] + nabla * gradiente_local_neuronio * Y[c-1][i - 1]\n",
    "                \n",
    "gradientes_locais.append(gradientes_escondidos)\n",
    "\n",
    "I2, Y2 = forward(rede, dados[indice_entrada])\n",
    "\n",
    "print(Y2[-1])\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-5.04618388]), array([-0.99021739,  0.90467114])]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
